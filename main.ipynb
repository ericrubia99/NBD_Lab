{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networking for Big Data - Project\n",
    "- Jonas Barth 2050678\n",
    "- Susanna Bravi 1916681\n",
    "- Eric Rubia Aguilera 2049558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:20:54.135900300Z",
     "start_time": "2023-07-10T19:20:48.067743Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyshark\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import copy\n",
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "from functools import reduce\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all displayed dataframes interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:20:54.262904800Z",
     "start_time": "2023-07-10T19:20:48.551735600Z"
    }
   },
   "outputs": [],
   "source": [
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoreload external modules to avoid restarting the kernel every time changes are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:20:54.271465700Z",
     "start_time": "2023-07-10T19:20:49.108728900Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "#### 1. Extract general info from your trace using capinfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:20:54.403737300Z",
     "start_time": "2023-07-10T19:20:50.164729300Z"
    }
   },
   "outputs": [],
   "source": [
    "!capinfos -A data/packets.pcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Time Evaluation between Sequential and Parallel reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the time taken to process `.pcap` file sequentially and parallely as a function of the number of packets in the `.pcap` file. This allows us to show how the processing time scales with the number of packets. We initially used the `pyshark` library, however since it proved to be too slow to allow for a fast turnaround, we decided to use the `scapy` library instead which offers a similar functionality to `pyshark` but with better speed. Each processing algorithm is timed once, using Python's `timeit` package.\n",
    "   \n",
    "The [read.py](reading/) script lets you time either a sequential or parallel algorithm for a single `.pcap` file. The results are stored in a `.feather` file.,\n",
    "\n",
    "We run the processing for the following number of packets:,\n",
    "\n",
    "* 10\n",
    "* 100\n",
    "* 1000\n",
    "* 10000\n",
    "* 100000\n",
    "* 1000000\n",
    "\n",
    "##### Sequential Processing,\n",
    "The sequential processing, opens the `.pcap` file, reads the packets one by one into domain objects, creates a dataframe, and closes the file.,\n",
    "\n",
    "##### Parallel Processing,\n",
    "The parallel processing algorithm first divides the given `.pcap` file into $n$ smaller `.pcap` files of $p$ packets each. Then, a maximum of $m$ processes are started in parallel, each of which processes a single `.pcap` file sequentially. The number of parallel processes is capped at the parameter $m$ as not to overwhelm the available computing resources.,\n",
    "\n",
    "#### Results,\n",
    "The sequential and parallel processing algorithms were run once for each `.pcap` file. The line plot below shows the processing time in seconds for the number of packets in the `.pcap` files. For files with a number of packets $\\le 10000$, the parallel and sequential reading takes more or less the same amount of time. For the largest file of $1000000$ packets, the parallel processing is much faster than the sequential processing.,\n",
    "\n",
    "With parallel processing, there is some overhead in splitting the original file and starting the processes, hence we can expect that the pay-off of parallel processing to be insignificant for a small number of packets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the saved timing data and split the times into a sequential and parallel group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:20:54.661169300Z",
     "start_time": "2023-07-10T19:20:54.139901300Z"
    }
   },
   "outputs": [],
   "source": [
    "timing_df = pd.read_feather(\"data/reading_times.feather\")\n",
    "parallel_times = timing_df[timing_df.read_type == 'parallel']\n",
    "sequential_times = timing_df[timing_df.read_type == 'sequential']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot using the function from our own `plot` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:20:59.660503200Z",
     "start_time": "2023-07-10T19:20:54.271465700Z"
    }
   },
   "outputs": [],
   "source": [
    "import plot\n",
    "fig, _ = plot.plot_read_time(sequential_times, parallel_times)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract the IP which generates the highest amount as sender traffic, evaluate the bit rate (0.1 sec) for the 6 IP addresses mostly used as endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have close to a million packets in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:03.124375900Z",
     "start_time": "2023-07-10T19:20:59.660503200Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_feather(\"data/packets_df.feather\")\n",
    "len(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Sender Traffic\n",
    "\n",
    "Count and sort by the number of packets sent from each source address, to find the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:03.332177100Z",
     "start_time": "2023-07-10T19:21:02.427411400Z"
    }
   },
   "outputs": [],
   "source": [
    "ip_grouped = data_frame.groupby([\"IP_SRC\"], as_index=False)[['length']].agg('sum')\n",
    "ip_grouped.sort_values(by=['length'], ascending=False, inplace=True)\n",
    "top_10_ip_src = ip_grouped.head(10)\n",
    "max_ip = ip_grouped.iloc[0].IP_SRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the top 10 senders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:06.052684300Z",
     "start_time": "2023-07-10T19:21:03.363433800Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.bar(x=top_10_ip_src.IP_SRC, height=top_10_ip_src.length, tick_label=top_10_ip_src.IP_SRC)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Top 10 Senders: IP Addresses\")\n",
    "ax.set_yticks([10**y for y in range(6, 9)], [f\"$10^{y}$\" for y in range(6, 9)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Bit Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finding the top 6 destination addresses, we group on the `IP_DST` column and sum the number of found rows. Then, we sort and pick the top 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:06.520664300Z",
     "start_time": "2023-07-10T19:21:06.063673800Z"
    }
   },
   "outputs": [],
   "source": [
    "max_ip_data = data_frame[data_frame.IP_SRC == max_ip]\n",
    "top_6_ip_dest = max_ip_data.groupby([\"IP_DST\"], as_index=False)[['length']].agg('sum').sort_values(by=['length'], ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the top 6 destination addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:08.032991900Z",
     "start_time": "2023-07-10T19:21:06.349598300Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.bar(x=top_6_ip_dest.IP_DST, height=top_6_ip_dest.length, tick_label=top_6_ip_dest.IP_DST)\n",
    "ax.set_title(\"Top 6 Destinations: IP Addresses\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:13.074290500Z",
     "start_time": "2023-07-10T19:21:08.041000900Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(data_frame[\"length\"], x='length',\n",
    "                   nbins=15, \n",
    "                   title='Histogram of Packet Length',\n",
    "                   labels={'length':'Packet Length (Byte)'},\n",
    "                   opacity=0.8,\n",
    "                   log_y=False,\n",
    "                   color_discrete_sequence=['#2a9d8f'],\n",
    "                   text_auto=True,\n",
    "                   template='plotly_white',\n",
    "                   width=800, \n",
    "                   height=400)\n",
    "fig.update_layout(\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    #margin=dict(l=20, r=20, t=20, b=20),\n",
    "    #paper_bgcolor=\"gray\"\n",
    ")\n",
    "fig.update_traces(textfont_size=9, textangle=0, textposition=\"outside\", cliponaxis=False)\n",
    "fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the bit rate for the top 6 destination addresses of the IP that sends the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:19.011653800Z",
     "start_time": "2023-07-10T19:21:13.074290500Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot import plot_bit_rate\n",
    "\n",
    "plot_bit_rate(top_6_ip_dest, max_ip_data, max_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Top 5 Destination IP (received bytes) and Top 5 Source IP (sent bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:20.750638300Z",
     "start_time": "2023-07-10T19:21:19.025653300Z"
    }
   },
   "outputs": [],
   "source": [
    "top_5_ip_dest = data_frame.groupby([\"IP_DST\"])[['length']].agg('sum').sort_values(by=['length'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:21.350083700Z",
     "start_time": "2023-07-10T19:21:20.668858200Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot import plot_bar_addresses\n",
    "\n",
    "plot_bar_addresses(top_5_ip_dest, title='Top 5 IP Destination Addresses', ylabel='Destinations', xlabel='Received Bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:22.234793Z",
     "start_time": "2023-07-10T19:21:21.334452900Z"
    }
   },
   "outputs": [],
   "source": [
    "top_5_ip_src = data_frame.groupby([\"IP_SRC\"])[['length']].agg('sum').sort_values(by=['length'], ascending=False).head(5)\n",
    "plot_bar_addresses(top_5_ip_src, title='Top 5 Source Addresses', xlabel='Bytes Sent', ylabel='Sources')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate bitRate considering all the trace with 3 different sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:26.379297200Z",
     "start_time": "2023-07-10T19:21:22.172294200Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot import plot_total_bit_rate\n",
    "\n",
    "plot_total_bit_rate(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. GeoLocal Referenciation of the 5 sessions with the highest amount of traffic generated\n",
    "A session is defined as a tuple of a source IP and destination IP address. We count the number of these tuples in the data frame to determine the top 5 sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:26.995296Z",
     "start_time": "2023-07-10T19:21:26.378295400Z"
    }
   },
   "outputs": [],
   "source": [
    "from ip2geotools.databases.noncommercial import DbIpCity\n",
    "import folium\n",
    "from util import geo_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the source and destination IPs of the top 5 sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:29.411296800Z",
     "start_time": "2023-07-10T19:21:27.005297700Z"
    }
   },
   "outputs": [],
   "source": [
    "ip_grouped = copy.deepcopy(data_frame)\n",
    "df_srcdst = list(zip(ip_grouped.IP_SRC, ip_grouped.IP_DST))\n",
    "top_sessions = Counter(df_srcdst).most_common(5)\n",
    "src_ips, dst_ips = zip(*map(lambda a: (a[0][0], a[0][1]), top_sessions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the longitude and latitude of the IP pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:35.915297900Z",
     "start_time": "2023-07-10T19:21:29.422297300Z"
    }
   },
   "outputs": [],
   "source": [
    "src_geo, dst_geo = geo_infos(src_ips, dst_ips)\n",
    "\n",
    "src_geo = pd.DataFrame(src_geo, columns=['latitude', 'longitude', 'region'])\n",
    "dst_geo = pd.DataFrame(dst_geo, columns=['latitude', 'longitude', 'region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a world map and connect the source and destination locations for each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:36.610297900Z",
     "start_time": "2023-07-10T19:21:35.983296800Z"
    }
   },
   "outputs": [],
   "source": [
    "flow_map = folium.Map([0, 0], zoom_start=2, tiles='Stamen Terrain')\n",
    "\n",
    "for i in range(len(src_geo)):\n",
    "    folium.Marker([src_geo.loc[i][0], src_geo.loc[i][1]], popup='<i>Mt. Hood Meadows</i>',\n",
    "                icon=folium.Icon(color='green')).add_to(flow_map)\n",
    "    folium.Marker([dst_geo.loc[i][0], dst_geo.loc[i][1]], popup='<i>Mt. Hood Meadows</i>',\n",
    "                icon=folium.Icon(color='red')).add_to(flow_map)\n",
    "    folium.PolyLine([(src_geo.loc[i][0], src_geo.loc[i][1]), (dst_geo.loc[i][0], dst_geo.loc[i][1])],\n",
    "                  color=\"blue\", weight=1.5, opacity=1).add_to(flow_map)\n",
    "\n",
    "flow_map.save(\"./Map_top_5_flows.html\")\n",
    "display(flow_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Flow Analysis - 10 Protocol mostly used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the most common protocols, we need to first find the flows in the data. A flow is a quintuplet of `source ip, destination ip, source port, destination port, protocol`. We then count the number of items in each group and sum them by protocol to get the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:40.725450100Z",
     "start_time": "2023-07-10T19:21:36.582295700Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_flows = data_frame.groupby(['IP_SRC', 'IP_DST', 'Protocol', 'src_port', 'dst_port']).agg(tot_len = pd.NamedAgg(column = 'length', aggfunc = 'sum')).reset_index()\n",
    "number_to_protocol = {1:\"ICMP\", 6:\"TCP\", 17:\"UDP\", 50:\"ESP\", 4:\"IPv4\", 47:\"GRE\", 89:\"OSPFIGP\", 97:\"ETHERIP\", 103:\"PIM\"}\n",
    "grouped_flows[\"Protocol\"] = grouped_flows[\"Protocol\"].replace(number_to_protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the most commonly used protocols in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:41.415018700Z",
     "start_time": "2023-07-10T19:21:40.412952700Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot import plot_protocol_count\n",
    "\n",
    "plot_protocol_count(grouped_flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Port Scanner evaluation (10 Ports mostly used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:41.835801200Z",
     "start_time": "2023-07-10T19:21:41.418384900Z"
    }
   },
   "outputs": [],
   "source": [
    "def port_count(ports):\n",
    "    \"\"\"Count the occurrences of well-known ports and return them as a dictionary.\"\"\"\n",
    "    well_known_ports = ports[ports < 1024]\n",
    "    well_known_ports_count = well_known_ports.value_counts().reset_index()\n",
    "    well_known_ports_count.columns = ['port', 'count']\n",
    "    return well_known_ports_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of occurrences of each port and sort them in descending order for the bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:42.313998900Z",
     "start_time": "2023-07-10T19:21:41.835801200Z"
    }
   },
   "outputs": [],
   "source": [
    "source_ports = port_count(data_frame[\"src_port\"])\n",
    "dest_ports = port_count(data_frame[\"dst_port\"])\n",
    "\n",
    "source_ports.sort_values(by='count', ascending=False, inplace=True)\n",
    "dest_ports.sort_values(by='count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the protocol name to the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:42.756784400Z",
     "start_time": "2023-07-10T19:21:42.298371500Z"
    }
   },
   "outputs": [],
   "source": [
    "port_number_to_name = {443:\"443 - HTTPS\",80:\"80 - HTTP\",-1:\"-1 - ICMP\",53:\"53 - DNS\",873:\"873 - rsync\",993:\"993 - IMAP4\",22:\"22 -SSH\",161:\"161 - SNMP \",25:\"25 - SMTP\",123:\"123 - NTP\"}\n",
    "\n",
    "dports_10 = dest_ports.sort_values(by=['count'], ascending=False).head(10)\n",
    "dports_10[\"port\"] = dports_10[\"port\"].replace(port_number_to_name)\n",
    "\n",
    "sports_10 = source_ports.sort_values(by=['count'], ascending=False).head(10)\n",
    "sports_10[\"port\"] = sports_10[\"port\"].replace(port_number_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the destination ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:43.449782300Z",
     "start_time": "2023-07-10T19:21:42.760778900Z"
    }
   },
   "outputs": [],
   "source": [
    "port_plot = px.bar(dports_10,x=dports_10['port'],y=dports_10['count'],text_auto=True,\n",
    "                    title='Top 10 Destination ports used',\n",
    "                    opacity=0.8,\n",
    "                    color_discrete_sequence=['#86bbd8'],\n",
    "                    template='plotly_white',\n",
    "                    width=800, \n",
    "                    height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the source ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:46.672399Z",
     "start_time": "2023-07-10T19:21:43.448780800Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_plot = px.bar(sports_10,x=sports_10['port'],y=sports_10['count'],text_auto=True,\n",
    "                    title='Top 10 Source ports used',\n",
    "                    opacity=0.8,\n",
    "                    color_discrete_sequence=['#86bbd8'],\n",
    "                    template='plotly_white',\n",
    "                    width=800, \n",
    "                    height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:21:48.183394200Z",
     "start_time": "2023-07-10T19:21:46.768401400Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Destination Ports\", \"Source Ports\"),shared_yaxes=True)\n",
    "fig.add_trace(port_plot['data'][0], row=1, col=1)\n",
    "fig.add_trace(sport_plot['data'][0], row=1, col=2)\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=1500,\n",
    "    title_text=\"10 most used ports\",\n",
    "    xaxis_title_text ='Ports',\n",
    "    yaxis_title_text ='Count')\n",
    "fig.update_layout(template='plotly_white')\n",
    "fig.update_xaxes(tickfont_family=\"Arial Black\")\n",
    "fig.update_xaxes(title_text=\"ports\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=2)\n",
    "fig.update_traces(textfont_size=9, textangle=0, textposition=\"outside\", cliponaxis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. InterArrival Time boxplot between TCP and UDP Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:30:16.267877100Z",
     "start_time": "2023-07-10T19:30:13.037998400Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "def inter_arrival_time(data):\n",
    "    val = np.array(data[\"time\"])\n",
    "    #Calculate the n-th discrete difference along the given axis\n",
    "    return np.diff(val)\n",
    "\n",
    "data_protocol = copy.deepcopy(data_frame[data_frame[\"Protocol\"].isin([6,17])])\n",
    "data_protocol[\"Protocol\"] = data_protocol[\"Protocol\"].replace({1:\"ICMP\",6:\"TCP\",17:\"UDP\"})\n",
    "\n",
    "print(Counter(data_protocol[\"Protocol\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:30:19.420268600Z",
     "start_time": "2023-07-10T19:30:16.322046Z"
    }
   },
   "outputs": [],
   "source": [
    "box_len = px.box(data_protocol, y=\"length\", x='Protocol', color='Protocol', template='plotly_white',color_discrete_sequence=[ '#e5b769' ,'#2a9d8f'])\n",
    "#Seems like we have small pkts\n",
    "#From the histogram at the beginning of the document we can see that almost all the pkts are < 2000 byte\n",
    "#Let's try to do for only pkts of size smaller than the quantile 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:30:22.527412700Z",
     "start_time": "2023-07-10T19:30:19.433298700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_protocol_2000 = data_protocol[data_protocol[\"length\"]<= 2860]\n",
    "box_len_2000 = px.box(data_protocol_2000, y=\"length\", x='Protocol', color='Protocol',template='plotly_white',color_discrete_sequence=[ '#e5b769' ,'#2a9d8f'])\n",
    "# The TCP box is quite big so the interquantile range is large... so we have data that is quite variable\n",
    "# The UDP's box is smaller that the TCP one so here the variance is smaller and also the size of the pckts is smaller that the TCP. UDP also does not have pckts pf size bigger that 1500 byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:30:23.504274800Z",
     "start_time": "2023-07-10T19:30:22.997264600Z"
    }
   },
   "outputs": [],
   "source": [
    "np.quantile(np.array(data_protocol['length']),q=0.9) #our quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:30:39.336922400Z",
     "start_time": "2023-07-10T19:30:26.862102500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=4,subplot_titles=(\"Original data UDP\",\"Original data TCP\", \"Pckts whit length < 2860 bytes UDP\",\"Pckts whit length < 2860 bytes TCP\"))\n",
    "fig.add_trace(box_len['data'][0], row=1, col=1)\n",
    "fig.add_trace(box_len['data'][1], row=1, col=2)\n",
    "fig.add_trace(box_len_2000['data'][0], row=1, col=3)\n",
    "fig.add_trace(box_len_2000['data'][1], row=1, col=4)\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=1500,\n",
    "    title_text=\"Packets length\")\n",
    "fig.update_layout(template='plotly_white',showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no UDP packets bigger than 1500 bytes, which is the Maximum Transmission Unit (MTU) on the link layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "soloUDP = data_protocol[data_protocol[\"Protocol\"]=='UDP']\n",
    "np.max(np.array(soloUDP['length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inter arrival time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "tcp_data = data_protocol_2000[data_protocol_2000[\"Protocol\"]==\"TCP\"]\n",
    "udp_data = data_protocol_2000[data_protocol_2000[\"Protocol\"]==\"UDP\"]\n",
    "\n",
    "inteArr_TCP= []\n",
    "for elem in tcp_data.groupby(['IP_SRC', 'IP_DST', 'Protocol', 'src_port', 'dst_port']):\n",
    "    #groupby tuple (key,dataframe)\n",
    "    inteArr_TCP += inter_arrival_time(elem[1]).tolist()\n",
    "\n",
    "inteArr_UDP = []\n",
    "for elem in udp_data.groupby(['IP_SRC', 'IP_DST', 'Protocol', 'src_port', 'dst_port']):\n",
    "    inteArr_UDP += inter_arrival_time(elem[1]).tolist()\n",
    "\n",
    "\n",
    "val_ = inteArr_TCP + inteArr_UDP\n",
    "\n",
    "label_TCP = [ \"TCP\" for i in range(len(inteArr_TCP))]\n",
    "label_UDP =[ \"UDP\" for i in range(len(inteArr_UDP))]\n",
    "\n",
    "lab_ = label_TCP + label_UDP\n",
    "\n",
    "d = {'Protocol': lab_, 'IntArrTime': val_}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T19:30:48.856819100Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean InterArrivalTime TCP Session: %.4f\"% np.mean(np.array(inteArr_TCP)))\n",
    "print(\"Mean InterArrivalTime UDP Session: %.4f\"% np.mean(np.array(inteArr_UDP)))\n",
    "#the TCP interarrival time is smaller than the UDP\n",
    "#median?\n",
    "print(\"Median InterArrivalTime TCP Session: %.5f\"% np.median(np.array(inteArr_TCP)))\n",
    "print(\"Median InterArrivalTime UDP Session: %.5f\"% np.median(np.array(inteArr_UDP)))\n",
    "# 3rd quartile\n",
    "print(\"3rd quartile InterArrivalTime TCP Session: %.5f\"% np.quantile(np.array(inteArr_TCP),q=0.75))\n",
    "print(\"3rd quartile InterArrivalTime UDP Session: %.5f\"% np.quantile(np.array(inteArr_UDP),q=0.75))\n",
    "#After this make sense to plot it for values less that 0.00004 (median of TCP) seconds and 0.0009 (median of UDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:36:44.168131Z",
     "start_time": "2023-07-10T19:36:43.794133400Z"
    }
   },
   "outputs": [],
   "source": [
    "tcp_median = round(np.median(np.array(inteArr_TCP)),7)\n",
    "udp_median = round(np.median(np.array(inteArr_UDP)),7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T19:36:47.977791700Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1 = df[df[\"IntArrTime\"] <  udp_median]\n",
    "fig1 = px.box(df1, y=\"IntArrTime\", x='Protocol', color='Protocol', template='plotly_white',color_discrete_sequence=[ '#e5b769' ,'#2a9d8f'])\n",
    "#fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df2 = df[df[\"IntArrTime\"] < tcp_median]\n",
    "fig2 = px.box(df2, y=\"IntArrTime\", x='Protocol', color='Protocol', template='plotly_white',color_discrete_sequence=[ '#e5b769' ,'#2a9d8f'])\n",
    "#fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=4,subplot_titles=(f\"TCP < {udp_median} s\", f\"UDP < {udp_median} s\",f\"TCP < {tcp_median}\", f\"UDP < {tcp_median}\"))\n",
    "fig.add_trace(fig1['data'][0], row=1, col=1)\n",
    "fig.add_trace(fig1['data'][1], row=1, col=2)\n",
    "fig.add_trace(fig2['data'][0], row=1, col=3)\n",
    "fig.add_trace(fig2['data'][1], row=1, col=4)\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=1500,\n",
    "    title_text=\"Inter Arrival Time\",\n",
    "    yaxis_title_text='Time (s)')\n",
    "fig.update_layout(template='plotly_white',showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Develop your own analysis (e.g. Topology of the network using networkx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from networkx.algorithms import approximation as apx\n",
    "random.seed(26111998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dest_IPs, Source_IPs = list(data_frame['IP_DST']),list(data_frame['IP_SRC'])\n",
    "len(np.unique(Dest_IPs+Source_IPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_udp=data_frame[data_frame['Protocol']==17]\n",
    "data_tcp=data_frame[data_frame['Protocol']==6]\n",
    "data_ICMP=data_frame[data_frame['Protocol']==1]\n",
    "data_transport=data_frame[(data_frame['Protocol']==17) | (data_frame['Protocol']==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dest_IPs_UDP,Source_IPs_UDP=list(data_udp['IP_DST']),list(data_udp['IP_SRC'])\n",
    "print(\"There are \",len(np.unique(Dest_IPs_UDP+Source_IPs_UDP)),\" nodes sending and receiving UDP pckts\")\n",
    "Dest_IPs_TCP,Source_IPs_TCP=list(data_tcp['IP_DST']),list(data_tcp['IP_SRC'])\n",
    "print(\"There are \",len(np.unique(Dest_IPs_TCP+Source_IPs_TCP)),\" nodes sending and receiving TCP pckts\")\n",
    "Dest_IPs_ICMP,Source_IPs_ICMP=list(data_ICMP['IP_DST']),list(data_ICMP['IP_SRC'])\n",
    "len(np.unique(Dest_IPs_ICMP+Source_IPs_ICMP))\n",
    "print(\"There are \",len(np.unique(Dest_IPs_ICMP+Source_IPs_ICMP)),\"nodes sending and receiving ICMP pckts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDP GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_udp = data_udp.groupby(['IP_SRC', 'IP_DST', 'src_port', 'dst_port']).first().reset_index()\n",
    "data_udp = data_udp[['IP_SRC', 'IP_DST', 'src_port', 'dst_port']]\n",
    "UDP=data_udp[['IP_SRC','IP_DST']]\n",
    "udp_count=dict(UDP.value_counts())\n",
    "l=[]\n",
    "for i in range(len(data_udp)):\n",
    "    l.append(udp_count[(data_udp.iloc[i]['IP_SRC'],data_udp.iloc[i]['IP_DST'])])\n",
    "data_udp['Num Flows']=l\n",
    "data_udp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_UDP=nx.DiGraph()\n",
    "\n",
    "for _,i in data_udp.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    if (node_a,node_b) in Graph_UDP.edges:\n",
    "        Graph_UDP.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'])]\n",
    "    else:\n",
    "        Graph_UDP.add_edge(node_a,node_b)\n",
    "        Graph_UDP.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_UDP.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCP GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tcp = data_tcp.groupby(['IP_SRC', 'IP_DST', 'src_port', 'dst_port']).first().reset_index()\n",
    "data_tcp = data_tcp[['IP_SRC', 'IP_DST', 'src_port', 'dst_port']]\n",
    "TCP=data_tcp[['IP_SRC','IP_DST']]\n",
    "tcp_count=dict(TCP.value_counts())\n",
    "l=[]\n",
    "for i in range(len(data_tcp)):\n",
    "    l.append(tcp_count[(data_tcp.iloc[i]['IP_SRC'],data_tcp.iloc[i]['IP_DST'])])\n",
    "data_tcp['Num Flows']=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_TCP = nx.DiGraph()\n",
    "\n",
    "for _,i in data_tcp.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    if (node_a,node_b) in Graph_TCP.edges:\n",
    "        Graph_TCP.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'])]\n",
    "    else:\n",
    "        Graph_TCP.add_edge(node_a,node_b)\n",
    "        Graph_TCP.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_TCP.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICMP GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ICMP = data_ICMP.groupby(['IP_SRC', 'IP_DST', 'src_port', 'dst_port']).first().reset_index()\n",
    "data_ICMP = data_ICMP[['IP_SRC', 'IP_DST', 'src_port', 'dst_port']]\n",
    "ICMP=data_ICMP[['IP_SRC','IP_DST']]\n",
    "ICMP_count=dict(ICMP.value_counts())\n",
    "l=[]\n",
    "for i in range(len(data_ICMP)):\n",
    "    l.append(ICMP_count[(data_ICMP.iloc[i]['IP_SRC'],data_ICMP.iloc[i]['IP_DST'])])\n",
    "data_ICMP['Num Flows']=l\n",
    "data_ICMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_ICMP = nx.DiGraph()\n",
    "\n",
    "for _,i in data_ICMP.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    if (node_a,node_b) in Graph_ICMP.edges:\n",
    "        Graph_ICMP.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'])]\n",
    "    else:\n",
    "        Graph_ICMP.add_edge(node_a,node_b)\n",
    "        #In this case we could simply go for a directed graph without attributes because the numflow is always 1 and the list is [-1,-1] for all\n",
    "        Graph_ICMP.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_ICMP.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the topology of the 3 graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_UDP=data_udp.sample(400)\n",
    "sample_TCP=data_tcp.sample(400)\n",
    "sample_ICMP=data_ICMP.sample(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_subset_UDP=nx.DiGraph()\n",
    "\n",
    "for _,i in sample_UDP.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    if (node_a,node_b) in Graph_subset_UDP.edges:\n",
    "        Graph_subset_UDP.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'])]\n",
    "    else:\n",
    "        Graph_subset_UDP.add_edge(node_a,node_b)\n",
    "        Graph_subset_UDP.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_subset_UDP.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'])]\n",
    "\n",
    "Graph_subset_TCP=nx.DiGraph()\n",
    "\n",
    "for _,i in sample_TCP.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    if (node_a,node_b) in Graph_subset_TCP.edges:\n",
    "        Graph_subset_TCP.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'])]\n",
    "    else:\n",
    "        Graph_subset_TCP.add_edge(node_a,node_b)\n",
    "        Graph_subset_TCP.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_subset_TCP.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'])]\n",
    "\n",
    "Graph_subset_ICMP=nx.DiGraph()\n",
    "\n",
    "for _,i in sample_ICMP.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    if (node_a,node_b) in Graph_subset_ICMP.edges:\n",
    "        Graph_subset_ICMP.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'])]\n",
    "    else:\n",
    "        Graph_subset_ICMP.add_edge(node_a,node_b)\n",
    "        Graph_subset_ICMP.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_subset_ICMP.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('_mpl-gallery')\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,4))\n",
    "nx.draw(Graph_subset_UDP,node_size = 20, width = 0.5, node_color = '#2a9d8f', ax=axes[0])\n",
    "nx.draw(Graph_subset_TCP,node_size = 20, width = 0.5, node_color = '#2a9d8f', ax=axes[1])\n",
    "nx.draw(Graph_subset_ICMP,node_size = 20, width = 0.5, node_color = '#2a9d8f', ax=axes[2])\n",
    "\n",
    "axes[0].set_title(\"UDP sample Graph\")\n",
    "axes[1].set_title(\"TCP sample Graph\")\n",
    "axes[2].set_title(\"ICMP sample Graph\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this overview of the topology we have decided to put together the TCP and UDP packets since the graphs do seem to be very similar, while the ICMP graph has a particular topology with fewer sources and more destinations than the other two protocols. <br>\n",
    "This is beacause the ICMP protocol is used for troubleshooting, the sources want to know if the destinations are reacheable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transport GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as we have just mentioned above we are going to build a single graph containing both the packets send using the TCP or UDP protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's merge them!\n",
    "data_transport = data_transport.groupby(['IP_SRC', 'IP_DST', 'src_port', 'dst_port',\"Protocol\"]).first().reset_index()\n",
    "data_transport = data_transport[['IP_SRC', 'IP_DST', 'src_port', 'dst_port',\"Protocol\"]]\n",
    "TR=data_transport[['IP_SRC','IP_DST']]\n",
    "tr_count=dict(TR.value_counts())\n",
    "l=[]\n",
    "for i in range(len(data_transport)):\n",
    "    l.append(tr_count[(data_transport.iloc[i]['IP_SRC'],data_transport.iloc[i]['IP_DST'])])\n",
    "data_transport['Num Flows']=l\n",
    "#data_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new graph for both TCP and UDP\n",
    "Graph_Transport=nx.DiGraph()\n",
    "prot={17:'UDP',6:'TCP'}\n",
    "for _,i in data_transport.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    p=prot[i.Protocol]\n",
    "    if (node_a,node_b) in Graph_Transport.edges:\n",
    "        Graph_Transport.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'],p)]\n",
    "    else:\n",
    "        Graph_Transport.add_edge(node_a,node_b)\n",
    "        Graph_Transport.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_Transport.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'],p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph_Transport.get_edge_data(\"95.36.218.85\",\"202.9.24.18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's observe a small sample of the graph obtained combining both UDP and TCP pkts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_sample = data_transport.sample(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(transport_sample.Protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_Subset_Transport=nx.DiGraph()\n",
    "prot={17:'UDP',6:'TCP'}\n",
    "for _,i in transport_sample.iterrows():\n",
    "    node_a=i['IP_SRC']\n",
    "    node_b=i['IP_DST']\n",
    "    p=prot[i.Protocol]\n",
    "    if (node_a,node_b) in Graph_Subset_Transport.edges:\n",
    "        Graph_Subset_Transport.edges[node_a,node_b]['List']+=[(i['src_port'],i['dst_port'],p)]\n",
    "    else:\n",
    "        Graph_Subset_Transport.add_edge(node_a,node_b)\n",
    "        Graph_Subset_Transport.edges[node_a,node_b]['Num Flow']=i['Num Flows']\n",
    "        Graph_Subset_Transport.edges[node_a,node_b]['List']=[(i['src_port'],i['dst_port'],p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "nx.draw(Graph_Subset_Transport,node_size = 20, width = 0.5, node_color = '#2a9d8f', font_size = 6,ax=ax)\n",
    "plt.title(\"Sample of the Transport Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have observed before, the overall topology is very similar to the two graphs that we saw before. Hence, we think that, in terms of the topological study, it makes more sense to study the entirety of the pkt that use TCP and UDP altogether. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1,e2=transport_sample[transport_sample['Num Flows']>1].iloc[0][['IP_SRC','IP_DST']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_Subset_Transport.get_edge_data(e1, e2)\n",
    "# Here we can observe how now every flow has both assigned the source and the destination port plus the protocol used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Metrics to obtain a better understanding of the Graphs' topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many nodes we have?\n",
    "print(\"Number of nodes: %.0f\"% nx.number_of_nodes(Graph_ICMP))\n",
    "print(\"Number of edges: %.0f\"% nx.number_of_edges(Graph_ICMP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree of the two Graphs\n",
    "Counter(nx.degree_histogram(Graph_ICMP)) #almost all the nodes have degree 0\n",
    "#The degree centrality for a node v is the fraction of nodes it is connected to.\n",
    "plt.style.use('_mpl-gallery')\n",
    "plt.rcParams['font.family'] = 'Serif'\n",
    "plt.figure(figsize=(4, 4))\n",
    "#fig, axes = plt.subplots(1,2, figsize=(15,4))\n",
    "icmp_hist = plt.hist(np.log(list(nx.degree_centrality(Graph_ICMP).values())),bins=30,label='ICMP')\n",
    "transport_hist = plt.hist(np.log(list(nx.degree_centrality(Graph_Transport).values())),bins=30,label='Transport')\n",
    "plt.xlabel(\"Degree Centrality\")\n",
    "plt.legend()\n",
    "plt.title(\"Degree in log-scale\")\n",
    "plt.show()\n",
    "# there are lot of nodes that are destination and have a degree centrality near 0.\n",
    "# Nonetheless there are very few nodes with higher degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of the degree of the Transport Graph is %.6f\"% np.mean(list(nx.degree_centrality(Graph_Transport).values())),\"and the variance is %.6f\"% np.var(list(nx.degree_centrality(Graph_Transport).values())))\n",
    "print(\"The mean of the degree of the ICMP Graph is %.6f\"% np.mean(list(nx.degree_centrality(Graph_ICMP).values())),\"and the variance is %.6f\"% np.var(list(nx.degree_centrality(Graph_ICMP).values())))\n",
    "print()\n",
    "ratio = np.var(list(nx.degree_centrality(Graph_ICMP).values()))/np.var(list(nx.degree_centrality(Graph_Transport).values()))\n",
    "print(\"The ratio between the two variances is %.2f\"% ratio ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many nodes we need to delete to have a disconnected graph?\", apx.node_connectivity(Graph_ICMP)) # The graph is not connected and we can not calculate algorithms like the longest path\n",
    "# Node connectivity is equal to the minimum number of nodes that must be removed to disconnect G or render it trivial. \n",
    "# By Mengerâ€™s theorem, this is equal to the number of node independent paths (paths that share no nodes other than source and target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graphs are disconnected and directed, in order to evaluate other metrics we construct the undirected versions of the two graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDIRECTED GRAPHS\n",
    "undirected_ICMP = nx.Graph()\n",
    "undirected_ICMP.add_edges_from(Graph_ICMP.edges())\n",
    "undirected_transport = nx.Graph()\n",
    "undirected_transport.add_edges_from(Graph_Transport.edges()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is the ICMP graph acyciclic?\", nx.is_directed_acyclic_graph(Graph_ICMP)) # so our graph have cycles and we can not do the longest path (also the other 2)\n",
    "print(\"Is the Transport graph acyciclic?\", nx.is_directed_acyclic_graph(Graph_Transport)) \n",
    "print(\"Are there some clusters in ICMP?\", apx.average_clustering(undirected_ICMP)) #no clutsers in ICMP\n",
    "print(\"Are there some clusters in Transport graph?\", apx.average_clustering(undirected_transport)) #neither for the transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of different components\n",
    "print(\"Number of Connected Components of ICMP Graph is\", nx.number_connected_components(undirected_ICMP), \"and the number of connected components of the Transport Graph is\",nx.number_connected_components(undirected_transport))\n",
    "#ok, now we can look at the set of nodes in the connected graph containing the source that send more (from point 3)\n",
    "print(\"The set of nodes in the component of the ICMP Graph cointaing node 150.57.136.251 are\", nx.node_connected_component(undirected_ICMP,'150.57.136.251')) #only 4 nodes\n",
    "print(\"The cardinality of set of nodes in the component of the Transport Graph cointaing node 150.57.136.251 are\", len(nx.node_connected_component(undirected_transport,'150.57.136.251'))) #lot of nodes, let's display how many they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can look at the diameter of the components\n",
    "# In graph theory, the diameter of a connected component refers to the longest shortest path between any two nodes within that component. \n",
    "# In other words, it measures the maximum number of edges that must be traversed to go from one node to another within the component.\n",
    "component_diameter_ICMP = []\n",
    "for component in nx.connected_components(undirected_ICMP):\n",
    "    component_diameter_ICMP.append(nx.diameter(undirected_ICMP.subgraph(component)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_diameter_T = []\n",
    "for component in nx.connected_components(undirected_transport):\n",
    "    component_diameter_T.append(nx.diameter(undirected_transport.subgraph(component)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the Bar Plot for both the ICMP and the Transport Diameters\n",
    "#ICMP\n",
    "ICMPx=list(range(min(component_diameter_ICMP),max(component_diameter_ICMP)+1))\n",
    "aux=dict(sorted(Counter(component_diameter_ICMP).items()))\n",
    "for i in ICMPx:\n",
    "    if i not in aux.keys():\n",
    "        aux[i]=0\n",
    "ICMPy=list(dict(sorted(aux.items())).values())\n",
    "\n",
    "#Transport\n",
    "Tx=list(range(min(component_diameter_T),max(component_diameter_T)+1))\n",
    "auxT=dict(sorted(Counter(component_diameter_T).items()))\n",
    "for i in Tx:\n",
    "    if i not in auxT.keys():\n",
    "        auxT[i]=0\n",
    "auxT[0]=0\n",
    "Tx.append(0)\n",
    "Tx=sorted(Tx)\n",
    "Ty=list(dict(sorted(auxT.items())).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T09:25:58.984907200Z",
     "start_time": "2023-07-10T09:25:55.331771800Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5),sharey=False)\n",
    "\n",
    "# Bar Plot ICMP diameter nel primo subplot\n",
    "ax1.bar(ICMPx, ICMPy, label=\"ICMP diameter\")\n",
    "ax1.set_xlabel(\"Diameter\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_title(\"Frequencies of ICMP Component's Diameters\")\n",
    "\n",
    "# Bar Plot Transport diameter nel secondo subplot\n",
    "ax2.bar(Tx,Ty, label=\"Transport diameter\", color='orange')\n",
    "ax2.set_xlabel(\"Diameter\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_title(\"Frequencies of Transport Component's Diameters - Log Scale\")\n",
    "ax2.set_xticks(range(len(Tx)), Tx)\n",
    "ax2.set_yscale(\"log\")\n",
    "#plt.savefig(\"out/\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:49:55.794634400Z",
     "start_time": "2023-07-10T16:49:48.141577300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kneed import KneeLocator\n",
    "import math\n",
    "import operator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T09:28:45.919704700Z",
     "start_time": "2023-07-10T09:28:45.021704300Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame[\"Label DSCP\"] = pd.to_numeric(data_frame[\"Label DSCP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T09:28:56.914882900Z",
     "start_time": "2023-07-10T09:28:48.915705400Z"
    }
   },
   "outputs": [],
   "source": [
    "dscp_tab = {0: \"BE\",\n",
    "            8: \"Priority\",\n",
    "            10: \"Priority\",\n",
    "            12: \"Priority\",\n",
    "            14: \"Priority\",\n",
    "            16: \"Immediate\",\n",
    "            18: \"Immediate\",\n",
    "            20: \"Immediate\",\n",
    "            22: \"Immediate\",\n",
    "            24: \"Flash voice\",\n",
    "            26: \"Flash voice\",\n",
    "            28: \"Flash voice\",\n",
    "            30: \"Flash voice\",\n",
    "            32: \"Flash Override\",\n",
    "            34: \"Flash Override\",\n",
    "            36: \"Flash Override\",\n",
    "            38: \"Flash Override\",\n",
    "            40: \"Critical voice RTP\",\n",
    "            46: \"Critical voice RTP\",\n",
    "            48: \"Internetwork control\",\n",
    "            56: \"Network Control\"\n",
    "            } \n",
    "\n",
    "data_frame = data_frame.replace({'Label DSCP': dscp_tab})\n",
    "data_frame = data_frame.replace({'Label DSCP': {\"Priority\":\"AF\",\"Immediate\":\"AF\",\"Flash voice\":\"AF\",\n",
    "                                \"Flash Override\":\"AF\",\"Critical voice RTP\":\"EF\",\n",
    "                                 \"Internetwork control\":\"CS6\",\"Network Control\":\"CS6\",\n",
    "                                 4:\"NotKnown\",2:\"NotKnown\",6:\"NotKnown\",7:\"NotKnown\",\n",
    "                                 1:\"NotKnown\",41:\"EF\",42:\"EF\",43:\"EF\",44:\"EF\",45:\"EF\",49:\"NotKnown\",54:\"NotKnown\",11:\"NotKnown\",50:\"NotKnown\",29:\"NotKnown\"}})\n",
    "\n",
    "print(\"DSCP Occurrences: \")\n",
    "print(dict(Counter(data_frame[\"Label DSCP\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T09:28:58.627881300Z",
     "start_time": "2023-07-10T09:28:58.537885800Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique = data_frame.drop_duplicates([\"IP_DST\",\"dst_port\"])\n",
    "\n",
    "#all possible (IP_0,port_0)\n",
    "flows_list = data_unique[[\"IP_DST\",\"dst_port\"]].values.tolist()\n",
    "\n",
    "dict_rows = {}\n",
    "for i in tqdm(range(len(flows_list))):\n",
    "    #extract all packets received by each specific couple IP dst, port destination\n",
    "    subdata = data_frame[(data_frame[\"IP_DST\"] == flows_list[i][0]) & (data_frame[\"dst_port\"] == flows_list[i][1])]\n",
    "    \n",
    "    #20 is just the length of our vector when we change the values in a logaritmic scale\n",
    "    #max 2**19 --> 524288 | This consideration depends on your dataset\n",
    "    length = np.zeros(21)\n",
    "    pkt = np.zeros(21)\n",
    "    \n",
    "    #At least 2 pkts received by this specific (IP_0,port_0)\n",
    "    if subdata.shape[0] >= 2:\n",
    "        \n",
    "        \n",
    "\n",
    "        #Check about the label, we want to be sure to analyze a couple with just 1 DSCP\n",
    "        #The vector that represents this element will have just one label\n",
    "        \n",
    "        if Counter(subdata[\"Label DSCP\"] == 1):\n",
    "        \n",
    "            dtu = subdata.drop_duplicates([\"IP_SRC\",\"src_port\"])\n",
    "            \n",
    "            list_couple_src = dtu[[\"IP_SRC\",\"src_port\"]].values.tolist()\n",
    "            # New features:\n",
    "            # Packet level: min, max, mean and variance of pkt lenght.\n",
    "            # TO DO:\n",
    "            # Flow level: statistics of number of packets per flow, number of bytes per flow, flow duration, interarrival times\n",
    "            # TCP connection level: statistics of packets per TCP connection, bytes per connection, connection duration\n",
    "\n",
    "            \n",
    "            minimum,maximum=np.infty,0\n",
    "\n",
    "            for elem in list_couple_src:\n",
    "                #Observe each element in the Neighborhood (N)\n",
    "                finaldata = subdata[(subdata[\"IP_SRC\"]==elem[0]) & (subdata[\"src_port\"]==elem[1])]\n",
    "\n",
    "                aux1,aux2=min(finaldata['length']),max(finaldata['length'])\n",
    "                if aux1<minimum:\n",
    "                    minimum=aux1\n",
    "                if aux2>maximum:\n",
    "                    maximum=aux2\n",
    "\n",
    "                mean = np.mean(finaldata['length'])\n",
    "                var = np.var(finaldata['length'])\n",
    "\n",
    "                #this is for avoiding -inf values because the log of 0 is not defined\n",
    "                if (var == 0):\n",
    "                    var = 1\n",
    "                if (mean == 0):\n",
    "                    mean = 1\n",
    "                    \n",
    "\n",
    "                #Number of packets\n",
    "                #Ex: pck = 245, log_{2}(245) = 7.94 --> ceil()--> 8 \n",
    "                #The range considered is (2**7,2**8] = (128,256]\n",
    "                length[math.ceil(math.log(finaldata.shape[0])/math.log(2))] += 1\n",
    "                \n",
    "                #Packet length analysis --> Byte\n",
    "                #extract each packet length\n",
    "                for index,row in finaldata.iterrows():\n",
    "                    pkt[math.ceil(math.log(row[\"length\"])/math.log(2))] += 1\n",
    "                    \n",
    "            # Normalization vector both for packets and bytes    \n",
    "            # Tring to transform the new features in the log2 scale\n",
    "            dict_rows[(flows_list[i][0],flows_list[i][1])] = [list(Counter(subdata[\"Label DSCP\"]).keys())[0],length/sum(length),pkt/sum(pkt),np.log2(maximum),np.log2(minimum),np.log2(mean),np.log2(var)]\n",
    "            \n",
    "        else:\n",
    "            #print(\"problem\")\n",
    "            break\n",
    "\n",
    "            \n",
    "#Save the data in a pickle file\n",
    "aux=pd.DataFrame.from_dict(dict_rows)\n",
    "pd.to_pickle(aux,'flows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:48:57.685508800Z",
     "start_time": "2023-07-10T16:48:53.181043500Z"
    }
   },
   "outputs": [],
   "source": [
    "dataFlow=pd.read_pickle('flows.pkl')\n",
    "dataFlow.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:50:11.365296Z",
     "start_time": "2023-07-10T16:50:10.650288700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_pandas = []        \n",
    "for k,val in dataFlow.items():\n",
    "    obs = []\n",
    "    obs.append(val[0])\n",
    "    obs.extend(val[1].tolist())\n",
    "    obs.extend(val[2].tolist())\n",
    "    obs.append(val[3])\n",
    "    obs.append(val[4])\n",
    "    obs.append(val[5])\n",
    "    obs.append(val[6])\n",
    "    data_pandas.append(obs)\n",
    "col = [\"Label\"]\n",
    "col.extend([\"X\"+str(i)for i in range(46)])\n",
    "dataUns = pd.DataFrame.from_records(data_pandas,columns=col )\n",
    "#Select just items with a string label and not numeric #mmmm we have all string \n",
    "#dataUns = dataUns[dataUns[\"Label\"].isin(['AF','BE', 'CS6','EF','NotKnown'])]\n",
    "#Useful to encode the label, it will be exploited at the end of the classification\n",
    "le = preprocessing.LabelEncoder() #here we are transforming \"BE\" and the other labels into numbers\n",
    "dataUns[\"Label\"]  = le.fit_transform(dataUns[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:50:56.860619100Z",
     "start_time": "2023-07-10T16:50:56.766627Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extract X,Y\n",
    "X = dataUns.iloc[:,1:]\n",
    "Y = dataUns.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step:\n",
    "# 1)Extract train and test from our starting dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:50:58.736798Z",
     "start_time": "2023-07-10T16:50:58.588789500Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Apply oversampling to rebalance in the training the number of occurrences\n",
    "Not oversampling the 1, is the BE, we have already loooots of them, make all balanced classes, same cardinality as class with label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:49:40.509746200Z",
     "start_time": "2023-07-10T17:49:32.540162600Z"
    }
   },
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy={0:sum(y_train==1),\n",
    "                                      2:sum(y_train==1),\n",
    "                                      4:sum(y_train==1),\n",
    "                                      3:sum(y_train==1)},k_neighbors=2) \n",
    "X_over, Y_over = oversample.fit_resample(x_train, y_train)\n",
    "\n",
    "before_oversampling = Counter(le.inverse_transform(y_train))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for ax, sample, title in zip(axes, [Counter(le.inverse_transform(y_train)), Counter(le.inverse_transform(Y_over))], [\"Before\", \"After\"]):\n",
    "    ax.bar(x=list(sample.keys()), height=sample.values())\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig.suptitle(\"Oversampling\")\n",
    "fig.savefig(\"out/oversampling.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:51:46.376488200Z",
     "start_time": "2023-07-10T16:51:46.283481200Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_kwargs = {\n",
    "    \"init\": \"k-means++\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 1000,\n",
    "    \"random_state\": 26111998\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:02.367635900Z",
     "start_time": "2023-07-10T16:51:47.883390700Z"
    }
   },
   "outputs": [],
   "source": [
    "#3)Find the optimal K (number of clusters) according to the training\n",
    "sse = []\n",
    "for k in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=k,**kmeans_kwargs)\n",
    "    kmeans.fit(X_over)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "kl = KneeLocator(\n",
    "    range(1, 15), sse, curve=\"convex\", direction=\"decreasing\", interp_method= \"interp1d\"\n",
    ")\n",
    "opt = kl.elbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:03.735634300Z",
     "start_time": "2023-07-10T16:52:02.381637700Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, 15), sse)\n",
    "plt.xticks(range(1, 15))\n",
    "plt.vlines(x=opt, ymin=0, ymax=max(sse), linestyles=\"dashed\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.savefig(\"out/elbow_method.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:03.747633700Z",
     "start_time": "2023-07-10T16:52:03.580633600Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extract the minimum in the convex curve \n",
    "kl = KneeLocator(\n",
    "    range(1, 15), sse, curve=\"convex\", direction=\"decreasing\", interp_method= \"interp1d\"\n",
    ")\n",
    "opt = kl.elbow \n",
    "print(\"Optimal number of clusters: \",opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:07.029186600Z",
     "start_time": "2023-07-10T16:52:03.712634900Z"
    }
   },
   "outputs": [],
   "source": [
    "#4) Apply this clustering to the test\n",
    "#Apply again K.Means with this specific number of clusters\n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=opt,\n",
    "    n_init=10,\n",
    "    max_iter=1000,\n",
    "    random_state=26111998\n",
    ")\n",
    "kmeans.fit(X_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:07.431180600Z",
     "start_time": "2023-07-10T16:52:06.919180400Z"
    }
   },
   "outputs": [],
   "source": [
    "#Observe the results\n",
    "#In each cluster finding the occurrences of the DSCP Labels\n",
    "\n",
    "dict_label_dscp = {}\n",
    "\n",
    "for i in list(set(kmeans.labels_)):\n",
    "    #print(sum(kmeans.labels_== i))\n",
    "    ind = []\n",
    "    for s, j in enumerate(kmeans.labels_):\n",
    "        if j == i:\n",
    "            ind.append(s) \n",
    "\n",
    "    print(\"Label: \",i)\n",
    "    stats = Counter(le.inverse_transform(Y_over[ind]))\n",
    "    print(stats)\n",
    "    print(max(stats.items(), key=operator.itemgetter(1))[0])\n",
    "    dict_label_dscp[i] = max(stats.items(), key=operator.itemgetter(1))[0]\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:28.198599100Z",
     "start_time": "2023-07-10T16:52:28.126602Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "pred = kmeans.predict(x_test)    \n",
    "prediction = [ dict_label_dscp[elem] for elem in pred ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:52:31.470796100Z",
     "start_time": "2023-07-10T16:52:31.313803700Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [ \"BE\", \"NotKnown\",\"AF\", \"EF\",\"CS6\"]\n",
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    \n",
    "    '''Confusion Matrix Evaluation'''\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.matshow(df_confusion, cmap=cmap,fignum=1) # imshow\n",
    "    \n",
    "    for (i, j), z in np.ndenumerate(df_confusion):\n",
    "        plt.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white'))\n",
    "    \n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45,fontsize = 13)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(tick_marks, df_confusion.index,fontsize = 13)\n",
    "    #plt.tight_layout()\n",
    "    #plt.ylabel(df_confusion.index.name)\n",
    "    #plt.xlabel(df_confusion.columns.name)\n",
    "    plt.ylabel(\"True\",fontsize = 18)\n",
    "    plt.xlabel(\"Predicted\",fontsize = 18)\n",
    "    plt.grid(False)\n",
    "    plt.savefig(\"out/confusion_matrix.png\", format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:53:09.902422Z",
     "start_time": "2023-07-10T16:53:07.851522600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix evaluation   \n",
    "confmatrix = confusion_matrix(le.inverse_transform(y_test), \n",
    "                              prediction,\n",
    "                              labels=labels)\n",
    "\n",
    "df_confusion = pd.DataFrame(confmatrix, index=labels, columns=labels)\n",
    "df_conf_norm = df_confusion.div(df_confusion.sum(axis=1),axis=0)\n",
    "\n",
    "plot_confusion_matrix(df_conf_norm,cmap=plt.cm.YlOrBr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:53:07.817514400Z",
     "start_time": "2023-07-10T16:52:32.685803400Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_silhouette_values = silhouette_samples(X_over, kmeans.labels_)\n",
    "print(\"Average silhouette score:\", sample_silhouette_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T16:53:10.744419300Z",
     "start_time": "2023-07-10T16:53:10.000420800Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = ['#ff6b35', '#f7c59f', '#efefd0', '#004e89', '#1a659e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:30:13.898350400Z",
     "start_time": "2023-07-10T17:30:13.761343300Z"
    }
   },
   "outputs": [],
   "source": [
    "def silhouette_plot(clusters):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    y_lower = 10\n",
    "\n",
    "    for i in range(clusters.n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[clusters.labels_ == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = colors[i % len(colors)]\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # Update the value for the next graph\n",
    "\n",
    "\n",
    "    # axes limits\n",
    "    ax.set_ylim(0, len(X_over) + (opt + 1) * 10)\n",
    "    ax.set_xlabel(\"Silhouette\")\n",
    "    ax.set_ylabel(\"Cluster\")\n",
    "    ax.set_title(\"Silhouette Scores\")\n",
    "\n",
    "    # Vertical blue line for the mean value\n",
    "    silhouette_avg = silhouette_score(X_over, clusters.labels_)\n",
    "    ax.axvline(x=silhouette_avg, color='#004e89', linestyle=\"--\")\n",
    "\n",
    "    # Text for the mean value\n",
    "    ax.text(silhouette_avg + 0.01, 0, \"Mean = {:.2f}\".format(silhouette_avg))\n",
    "    fig.savefig(f\"out/silhouette_plot_{clusters.n_clusters}.png\", format=\"png\")\n",
    "    # Show\n",
    "    return fig, ax, silhouette_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:36:02.491527100Z",
     "start_time": "2023-07-10T17:35:34.059898900Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = silhouette_plot(kmeans)[0]\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:45:57.276815600Z",
     "start_time": "2023-07-10T17:43:46.000364500Z"
    }
   },
   "outputs": [],
   "source": [
    "fits = []\n",
    "\n",
    "for num_clusters in range(2, 15):\n",
    "    kmeans = KMeans(\n",
    "        init=\"k-means++\",\n",
    "        n_clusters=num_clusters,\n",
    "        n_init=10,\n",
    "        max_iter=1000,\n",
    "        random_state=26111998\n",
    "    )\n",
    "    kmeans.fit(X_over)\n",
    "    fits.append(kmeans)\n",
    "\n",
    "\n",
    "avg_silhouette_scores = [silhouette_score(X_over, kmeans.labels_) for kmeans in fits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:46:03.928825800Z",
     "start_time": "2023-07-10T17:46:02.037823300Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(2, 15), avg_silhouette_scores)\n",
    "ax.set_xlabel(\"Clusters\")\n",
    "ax.set_ylabel(\"Average Silhouette Score\")\n",
    "ax.set_title(\"Average Silhouette Scores\")\n",
    "fig.savefig(\"out/avg_silhouette.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies Bouldin Index\n",
    "The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.\n",
    "\n",
    "The minimum score is zero, with lower values indicating better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:42:17.311856500Z",
     "start_time": "2023-07-10T17:41:53.177341800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "def cluster_scores(score_func):\n",
    "    db_scores = []\n",
    "    n_clusters = range(2, 15)\n",
    "    for k in n_clusters:\n",
    "        kmeans = KMeans(n_clusters=k,**kmeans_kwargs)\n",
    "        kmeans.fit(X_over)\n",
    "        db_score = score_func(X_over, kmeans.labels_)\n",
    "        db_scores.append(db_score)\n",
    "\n",
    "    return n_clusters, db_scores\n",
    "\n",
    "def cluster_score_plot(n_clusters, scores, title):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(n_clusters, scores)\n",
    "    ax.set_xlabel(\"Number of Clusters\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(title)\n",
    "    return fig, ax\n",
    "\n",
    "n_clusters, db_scores = cluster_scores(davies_bouldin_score)\n",
    "fig, _ = cluster_score_plot(n_clusters, db_scores, \"Davies Bouldin\")\n",
    "fig.savefig(\"out/davies_bouldin.png\", format=\"png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski Harabasz score\n",
    "The score is defined as ratio of the sum of between-cluster dispersion and of within-cluster dispersion. It is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Here cohesion is estimated based on the distances from the data points in a cluster to its cluster centroid and separation is based on the distance of the cluster centroids from the global centroid.\n",
    "\n",
    "Higher value of CH index means the clusters are dense and well separated, though there is no rule for choosing the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:42:29.464400400Z",
     "start_time": "2023-07-10T17:42:17.316824500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "n_clusters, ch_scores = cluster_scores(calinski_harabasz_score)\n",
    "fig, _ = cluster_score_plot(n_clusters, ch_scores, \"Calinski Harabasz\")\n",
    "fig.savefig(\"out/calinski_harabasz.png\", format=\"png\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
